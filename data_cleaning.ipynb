{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions Episodes Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romai\\AppData\\Local\\Temp\\ipykernel_29120\\3862019814.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new_filtered.rename(columns={'created': 'created_at', 'vote_key': 'emotion_vote_id'}, inplace=True)\n",
      "C:\\Users\\romai\\AppData\\Local\\Temp\\ipykernel_29120\\3862019814.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new_filtered['created_at'] = df_new_filtered['created_at'].str.replace(' +0000 UTC', '')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = './raw_data/emotions_episode_votes.csv'\n",
    "file_path_new = './data/emotions_episode_votes_modified.csv'\n",
    "\n",
    "# Attempt to correctly parse the CSV file by detecting the delimiter automatically\n",
    "df_new_corrected = pd.read_csv(file_path, sep=None, engine='python')\n",
    "\n",
    "# Convert the column names to lowercase\n",
    "df_new_corrected.columns = map(str.lower, df_new_corrected.columns)\n",
    "\n",
    "# Retain only the specified columns\n",
    "columns_to_keep_corrected = ['episode_id', 'emotion_id', 'user_id', 'created', 'vote_key']\n",
    "df_new_filtered = df_new_corrected[columns_to_keep_corrected]\n",
    "\n",
    "# Rename created to created_at and vote_key to emotion_vote_id\n",
    "df_new_filtered.rename(columns={'created': 'created_at', 'vote_key': 'emotion_vote_id'}, inplace=True)\n",
    "df_new_filtered['created_at'] = df_new_filtered['created_at'].str.replace(' +0000 UTC', '')\n",
    "\n",
    "# Display the modified dataframe\n",
    "df_new_filtered.head()\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df_new_filtered.to_csv(file_path_new, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episode Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = './raw_data/episode_comment.csv'\n",
    "file_path_new = './data/episode_comment_modified.csv'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path, delimiter=',')\n",
    "\n",
    "# Filter the rows where depth is greater than 0 - we don't want the replies to the comments\n",
    "df_filtered = df[df['depth'] < 1]\n",
    "\n",
    "# Select only specified columns\n",
    "df_filtered = df_filtered[['id', 'episode_id', 'tv_show_name', 'episode_season_number', 'episode_number', 'user_id', 'comment', 'nb_likes', 'created_at', 'updated_at']]\n",
    "\n",
    "# Rename id to comment_id\n",
    "df_filtered.rename(columns={'id': 'comment_id'}, inplace=True)\n",
    "\n",
    "df_filtered.head()\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df_filtered.to_csv(file_path_new, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Followed TV Show preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = './raw_data/followed_tv_show.csv'\n",
    "file_path_new = './data/followed_tv_show_modified.csv'\n",
    "\n",
    "# Attempt to correctly parse the CSV file by detecting the delimiter automatically\n",
    "df_new_corrected = pd.read_csv(file_path, sep=None, engine='python')\n",
    "\n",
    "# Display the corrected dataframe structure to verify the column names and data\n",
    "df_new_corrected.head()\n",
    "\n",
    "# Retain only the specified columns\n",
    "columns_to_keep_corrected = ['tv_show_name', 'tv_show_id', 'created_at', 'active', 'archived', 'user_id']\n",
    "df_new_filtered = df_new_corrected[columns_to_keep_corrected]\n",
    "\n",
    "# Display the modified dataframe\n",
    "df_new_filtered.head()\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df_new_filtered.to_csv(file_path_new, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Episode preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define a mapping from order values to note values\n",
    "order_mapping = {'1': 1, '27': 2, '28': 3, '29': 4, '3': 5}\n",
    "\n",
    "# Function to convert order and RATING_ID to note\n",
    "def convert_to_note(row):\n",
    "    # Split the 'order' string into a list of strings, then convert to a list of integers\n",
    "    order_values = list(map(int, row['order'].split(',')))\n",
    "    # Get the index of RATING_ID in order_values, then use this index to find the corresponding note value\n",
    "    try:\n",
    "        rating_index = order_values.index(row['RATING_ID'])\n",
    "        return order_mapping[str(order_values[rating_index])]\n",
    "    except ValueError:\n",
    "        # In case the RATING_ID is not found in the order list, return NaN or some error indicator\n",
    "        return float('nan')\n",
    "\n",
    "file_path = './raw_data/ratings_episode_votes.csv'\n",
    "df = pd.read_csv(file_path, sep=None, engine='python')\n",
    "\n",
    "# Apply the conversion function to each row\n",
    "df['note'] = df.apply(convert_to_note, axis=1)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['order', 'RATING_ID', 'IS_DELETED', 'DB_UPDATE_TS', 'set']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "# Rename the column created to created_at\n",
    "df.rename(columns={'created': 'created_at'}, inplace=True)\n",
    "# Remove the +0000 UTC  from the created_at column +0000 UTC (2022-09-06 20:42:29 +0000 UTC to 2022-09-06 20:42:29)\n",
    "df['created_at'] = df['created_at'].str.replace(' +0000 UTC', '')\n",
    "\n",
    "# Display the modified dataframe\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "output_file_path = './data/ratings_episode_votes_modified.csv'\n",
    "df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seen Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romai\\AppData\\Local\\Temp\\ipykernel_5900\\2801304209.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "rewatched_episode_df = pd.read_csv('./raw_data/rewatched_episode.csv')\n",
    "seen_episode_df = pd.read_csv('./raw_data/seen_episode.csv')\n",
    "\n",
    "# Drop updated_at of seen_episode_df\n",
    "seen_episode_df.drop(columns=['updated_at', 'tweet_id'], inplace=True)\n",
    "# Drop created_at of rewatched_episode_df\n",
    "rewatched_episode_df.drop(columns=['created_at'], inplace=True)\n",
    "\n",
    "# Merge the datasets on specified columns\n",
    "merge_columns = ['tv_show_name', 'episode_season_number', 'episode_number', 'user_id', 'episode_id']\n",
    "final_df = pd.merge(seen_episode_df, rewatched_episode_df[merge_columns + ['cpt']], \n",
    "                    on=merge_columns, \n",
    "                    how='left')\n",
    "\n",
    "# Replace NaN in rewatched_count with 0\n",
    "final_df['rewatched_count'] = final_df['cpt'].fillna(0).astype(int)\n",
    "final_df.drop(columns=['cpt'], inplace=True)  # Drop the 'cpt' column as it's no longer needed\n",
    "\n",
    "# Remove the rows where tv_show_name is NaN or empty\n",
    "final_df = final_df[final_df['tv_show_name'].notna()]\n",
    "\n",
    "# # Select and rename relevant columns for the final dataset\n",
    "# final_dataset = final_df[['updated_at', 'tv_show_name', 'episode_season_number', 'episode_number', \n",
    "#                           'user_id', 'episode_id', 'created_at', 'rewatched_count']]\n",
    "\n",
    "# Display the first few rows of the final dataset\n",
    "final_df.head()\n",
    "\n",
    "# Save the final dataset to a new CSV file\n",
    "file_path_new = './data/seen_episode_modified.csv'\n",
    "final_df.to_csv(file_path_new, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
